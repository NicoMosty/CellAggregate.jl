{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA\n",
    "using DelimitedFiles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# idx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nearest_neighbors (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean(points,i,j) = sqrt((points[i,1]-points[j,1])^2+(points[i,2]-points[j,2])^2+(points[i,3]-points[j,3])^2)\n",
    "\n",
    "function dist_kernel!(idx, points ,r_max)\n",
    "    # Defining Index for kernel\n",
    "    i = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    j = (blockIdx().y - 1) * blockDim().y + threadIdx().y\n",
    "    \n",
    "    # Limiting data inside matrix\n",
    "    if i <= size(points, 1) && j <= size(points, 1)\n",
    "        if euclidean(points,i,j) < r_max\n",
    "            idx[i, j] = i\n",
    "        else\n",
    "            idx[i, j] = 0\n",
    "        end \n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function reduce_kernel(idx,idx_red,idx_sum)\n",
    "    # Defining Index for kernel\n",
    "    i  = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "\n",
    "    # Limiting data inside matrix\n",
    "    if i <= size(idx,1)\n",
    "        # Cleaning idx_sum\n",
    "        idx_sum[i] = 0\n",
    "        \n",
    "        # looping on each row for searching non-zero values\n",
    "        for j = 1:size(idx,1)\n",
    "            if idx[j,i] != 0\n",
    "                idx_sum[i] += 1\n",
    "                idx_red[idx_sum[i],i] = j\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function nearest_neighbors(idx, idx_red, idx_sum, points ,r_max)\n",
    "    # Calculating Distance Matrix\n",
    "    threads =(32,32)\n",
    "    blocks  =cld.(size(points,1),threads)\n",
    "    @cuda threads=threads blocks=blocks dist_kernel!(idx, points ,r_max)\n",
    "\n",
    "    # Reducing Distance Matrix to Nearest Neighbors\n",
    "    threads=1024\n",
    "    blocks=cld.(size(idx,1),threads)\n",
    "    @cuda threads=threads blocks=blocks reduce_kernel(idx,idx_red,idx_sum)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------- VARIABLES ----------------------\n",
      "R_Agg = 15 | R_Max = 2.5 | col_size_idx = 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2504×3 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n",
       " -1.5  -4.62  -13.88\n",
       "  0.5  -4.62  -13.88\n",
       "  2.5  -4.62  -13.88\n",
       " -4.5  -2.89  -13.88\n",
       " -2.5  -2.89  -13.88\n",
       " -0.5  -2.89  -13.88\n",
       "  1.5  -2.89  -13.88\n",
       "  3.5  -2.89  -13.88\n",
       " -5.5  -1.15  -13.88\n",
       " -3.5  -1.15  -13.88\n",
       " -1.5  -1.15  -13.88\n",
       "  0.5  -1.15  -13.88\n",
       "  2.5  -1.15  -13.88\n",
       "  ⋮           \n",
       " -1.5   1.15   13.88\n",
       "  0.5   1.15   13.88\n",
       "  2.5   1.15   13.88\n",
       "  4.5   1.15   13.88\n",
       " -4.5   2.89   13.88\n",
       " -2.5   2.89   13.88\n",
       " -0.5   2.89   13.88\n",
       "  1.5   2.89   13.88\n",
       "  3.5   2.89   13.88\n",
       " -1.5   4.62   13.88\n",
       "  0.5   4.62   13.88\n",
       "  2.5   4.62   13.88"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------- SIZES ------------------------\n",
      "Size → idx     = (2504, 2504)\n",
      "     → idx_sum = (1, 2504)\n",
      "     → idx_red = (13, 2504)\n",
      "----------------------- RESULTS ----------------------\n",
      "  0.480456 seconds (228.66 k CPU allocations: 14.925 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13×2504 CuArray{Int32, 2, CUDA.Mem.DeviceBuffer}:\n",
       "  1   1   2   4   1   1   2   3   4  …  2453  2454  2455  2460  2461  2462\n",
       "  2   2   3   5   4   2   3   7   9     2454  2455  2456  2461  2462  2463\n",
       "  5   3   7   9   5   5   6   8  10     2461  2462  2463  2468  2469  2470\n",
       "  6   6   8  10   6   6   7  13  15     2493  2494  2495  2498  2499  2500\n",
       " 36   7  38  41  10   7   8  14  49     2494  2495  2496  2499  2500  2501\n",
       " 42  37  44  49  11  11  12  45  57  …  2498  2499  2500  2502  2502  2503\n",
       " 43  43  45  50  42  12  13  53  58     2499  2500  2501  2503  2503  2504\n",
       "  0  44   0   0  50  43  44  54   0     2500  2501  2504     0  2504     0\n",
       "  0   0   0   0  51  51  52   0   0     2502  2503     0     0     0     0\n",
       "  0   0   0   0   0  52  53   0   0     2503  2504     0     0     0     0\n",
       "  0   0   0   0   0   0   0   0   0  …     0     0     0     0     0     0\n",
       "  0   0   0   0   0   0   0   0   0        0     0     0     0     0     0\n",
       "  0   0   0   0   0   0   0   0   0        0     0     0     0     0     0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------- Memory Used ---------------------\n",
      "Effective GPU memory usage: 7.38% (147.812 MiB/1.955 GiB)\n",
      "No memory pool is in use."
     ]
    }
   ],
   "source": [
    "R_Agg=15\n",
    "r_max = 2.5\n",
    "idx_red_size =  r_max ≤ 2.80 ? 13 :\n",
    "            2.80 < r_max ≤ 3.45 ? 21 :\n",
    "            3.45 < r_max ≤ 3.80 ? 39 :\n",
    "            3.80 < r_max ≤ 4.00 ? 55 :\n",
    "            70\n",
    "X = Float64.(readdlm(\"../../../data/init/Sphere/$(R_Agg).xyz\")[3:end,2:end]) |> cu\n",
    "idx      = Int32.(zeros(size(X, 1), size(X, 1))) |> cu;\n",
    "idx_sum  = Int32.(zeros(1, size(idx, 1))) |> cu\n",
    "idx_red  = Int32.(zeros(idx_red_size, size(idx, 1))) |> cu\n",
    "\n",
    "println(\"--------------------- VARIABLES ----------------------\")\n",
    "println(\"R_Agg = $(R_Agg) | R_Max = $(r_max) | col_size_idx = $(idx_red_size)\")\n",
    "display(X)\n",
    "println(\"----------------------- SIZES ------------------------\")\n",
    "println(\"Size → idx     = $(size(idx))\")\n",
    "println(\"     → idx_sum = $(size(idx_sum))\")\n",
    "println(\"     → idx_red = $(size(idx_red))\")\n",
    "println(\"----------------------- RESULTS ----------------------\")\n",
    "CUDA.@time nearest_neighbors(idx, idx_red, idx_sum, X ,r_max)\n",
    "display(idx_red)\n",
    "println(\"--------------------- Memory Used ---------------------\")\n",
    "CUDA.memory_status()  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# forces"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sum_force! (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sum_force!(idx,points,force)\n",
    "    # Defining Index for kernel\n",
    "    i = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    k = (blockIdx().y - 1) * blockDim().y + threadIdx().y\n",
    "    \n",
    "    # Limiting data inside matrix\n",
    "    if i <= size(points, 1) && k <= size(points, 2)\n",
    "        # force[i,k] = points[i,k]\n",
    "        if idx_red[1,i] != 0\n",
    "            force[i,k] = 1\n",
    "        else\n",
    "            force[i,k] = 0\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "force = CUDA.zeros(size(X));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "InvalidIRError: compiling kernel #sum_force!(CuDeviceMatrix{Int32, 1}, CuDeviceMatrix{Float32, 1}, CuDeviceMatrix{Float32, 1}) resulted in invalid LLVM IR\n\u001b[31mReason: unsupported dynamic function invocation\u001b[39m\u001b[31m (call to getindex)\u001b[39m\nStacktrace:\n [1] \u001b[0m\u001b[1msum_force!\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mIn[10]:9\u001b[0m\n\u001b[31mReason: unsupported dynamic function invocation\u001b[39m\u001b[31m (call to !=)\u001b[39m\nStacktrace:\n [1] \u001b[0m\u001b[1msum_force!\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mIn[10]:9\u001b[0m\n\u001b[36m\u001b[1mHint\u001b[22m\u001b[39m\u001b[36m: catch this exception as `err` and call `code_typed(err; interactive = true)` to introspect the erronous code with Cthulhu.jl\u001b[39m",
     "output_type": "error",
     "traceback": [
      "InvalidIRError: compiling kernel #sum_force!(CuDeviceMatrix{Int32, 1}, CuDeviceMatrix{Float32, 1}, CuDeviceMatrix{Float32, 1}) resulted in invalid LLVM IR\n\u001b[31mReason: unsupported dynamic function invocation\u001b[39m\u001b[31m (call to getindex)\u001b[39m\nStacktrace:\n [1] \u001b[0m\u001b[1msum_force!\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mIn[10]:9\u001b[0m\n\u001b[31mReason: unsupported dynamic function invocation\u001b[39m\u001b[31m (call to !=)\u001b[39m\nStacktrace:\n [1] \u001b[0m\u001b[1msum_force!\u001b[22m\n\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mIn[10]:9\u001b[0m\n\u001b[36m\u001b[1mHint\u001b[22m\u001b[39m\u001b[36m: catch this exception as `err` and call `code_typed(err; interactive = true)` to introspect the erronous code with Cthulhu.jl\u001b[39m",
      "",
      "Stacktrace:",
      "  [1] check_ir(job::GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams, GPUCompiler.FunctionSpec{typeof(sum_force!), Tuple{CuDeviceMatrix{Int32, 1}, CuDeviceMatrix{Float32, 1}, CuDeviceMatrix{Float32, 1}}}}, args::LLVM.Module)",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/kb6yJ/src/validation.jl:141",
      "  [2] macro expansion",
      "    @ ~/.julia/packages/GPUCompiler/kb6yJ/src/driver.jl:418 [inlined]",
      "  [3] macro expansion",
      "    @ ~/.julia/packages/TimerOutputs/LHjFw/src/TimerOutput.jl:253 [inlined]",
      "  [4] macro expansion",
      "    @ ~/.julia/packages/GPUCompiler/kb6yJ/src/driver.jl:416 [inlined]",
      "  [5] emit_asm(job::GPUCompiler.CompilerJob, ir::LLVM.Module; strip::Bool, validate::Bool, format::LLVM.API.LLVMCodeGenFileType)",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/kb6yJ/src/utils.jl:83",
      "  [6] cufunction_compile(job::GPUCompiler.CompilerJob, ctx::LLVM.Context)",
      "    @ CUDA ~/.julia/packages/CUDA/ZdCxS/src/compiler/execution.jl:361",
      "  [7] #219",
      "    @ ~/.julia/packages/CUDA/ZdCxS/src/compiler/execution.jl:354 [inlined]",
      "  [8] JuliaContext(f::CUDA.var\"#219#220\"{GPUCompiler.CompilerJob{GPUCompiler.PTXCompilerTarget, CUDA.CUDACompilerParams, GPUCompiler.FunctionSpec{typeof(sum_force!), Tuple{CuDeviceMatrix{Int32, 1}, CuDeviceMatrix{Float32, 1}, CuDeviceMatrix{Float32, 1}}}}})",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/kb6yJ/src/driver.jl:76",
      "  [9] cufunction_compile(job::GPUCompiler.CompilerJob)",
      "    @ CUDA ~/.julia/packages/CUDA/ZdCxS/src/compiler/execution.jl:353",
      " [10] cached_compilation(cache::Dict{UInt64, Any}, job::GPUCompiler.CompilerJob, compiler::typeof(CUDA.cufunction_compile), linker::typeof(CUDA.cufunction_link))",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/kb6yJ/src/cache.jl:90",
      " [11] cufunction(f::typeof(sum_force!), tt::Type{Tuple{CuDeviceMatrix{Int32, 1}, CuDeviceMatrix{Float32, 1}, CuDeviceMatrix{Float32, 1}}}; name::Nothing, always_inline::Bool, kwargs::Base.Iterators.Pairs{Union{}, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})",
      "    @ CUDA ~/.julia/packages/CUDA/ZdCxS/src/compiler/execution.jl:306",
      " [12] cufunction(f::typeof(sum_force!), tt::Type{Tuple{CuDeviceMatrix{Int32, 1}, CuDeviceMatrix{Float32, 1}, CuDeviceMatrix{Float32, 1}}})",
      "    @ CUDA ~/.julia/packages/CUDA/ZdCxS/src/compiler/execution.jl:300",
      " [13] top-level scope",
      "    @ ~/.julia/packages/CUDA/ZdCxS/src/compiler/execution.jl:102"
     ]
    }
   ],
   "source": [
    "threads =(341,3)\n",
    "blocks  =cld.(size(X,1),threads)\n",
    "@cuda threads=threads blocks=blocks sum_force!(idx_red, X ,force)\n",
    "force"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.7",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
